model_list:
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      api_base: https://api.openai.com/v1
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_base: https://api.openai.com/v1
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_base: https://api.openai.com/v1
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini-2.5-flash
      api_base: https://generativelanguage.googleapis.com/
      api_key: os.environ/GOOGLE_API_KEY

environment_variables:
  OPENAI_API_KEY: ${OPENAI_API_KEY}

general_settings:
  completion_model: gpt-4o
  num_workers: 4  # Increase worker count for better concurrency
  cache: true  # Enable response caching for better performance
  store_prompts_in_spend_logs: true  # Store prompts in spend logs
  cache_params:
    host: localhost
    port: 6379
    type: redis  # Use Redis for distributed caching
  ttl: 3600  # Cache responses for 1 hour

# Logging configuration
logging:
  level: info  # Options: debug, info, warning, error
  log_requests: true
  log_responses: true

# Rate limiting to prevent API key exhaustion
rate_limits:
  enabled: true
  default_rpm: 60  # Default requests per minute
  default_tpm: 100000  # Default tokens per minute

litellm_settings:
  num_retries: 3
  request_timeout: 30
  callbacks: rag_pipeline.pre_request_hook.rag_handler_instance
  forward_client_headers_to_llm_api: true